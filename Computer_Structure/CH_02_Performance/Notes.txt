Designing for Performance
1. What is Performance?
    Performance = how fast a computer executes programs.
  Higher performance = less time to complete a task.

Often measured by:
  Response Time (Latency): Time taken to finish one task.

Measuring Performance

Execution Time (T):

`````````````````````ùëá  =(Instruction¬†Count) √ó(CPI)√ó (Clock¬†Cycle¬†Time)
                     T=(Instruction¬†Count)√ó(CPI)√ó(Clock¬†Cycle¬†Time)

            Instruction Count (IC): How many instructions a program needs.
            CPI (Cycles per Instruction): Average cycles per instruction.
            Clock Cycle Time: Inverse of clock rate (1 / frequency).

	‚Äã
Improving Performance

To make a computer faster, you can:
			Increase Clock Rate (GHz): Faster cycles ‚Üí faster CPU.
			‚ö† But leads to heating and power issues.
			Reduce CPI: Optimize hardware (pipelining, parallelism, better instruction design).
			Reduce Instruction Count (IC): Compiler optimizations, efficient ISA.

4. Benchmarks
			Standard programs used to compare systems (e.g., SPEC benchmarks).Real workloads are better indicators than synthetic ones.

5. Amdahl‚Äôs Law (Important)
			The speedup gained from improving part of a system is limited by the portion not improved.

Designing for Performance (Extended Notes)
		Falling Cost, Rising Power
		The cost of computer systems keeps dropping dramatically.Meanwhile, performance and capacity rise equally dramatically.
Example:	
		A laptop today ‚âà the power of an IBM mainframe from 10‚Äì15 years ago.What used to require a multi-million-dollar machine now fits in your backpack.


						Applications Driving Performance Needs
											Modern desktop applications demand the great power of today‚Äôs processors:
											Image Processing ‚Üí e.g., editing high-res photos, filters.
											3D Rendering ‚Üí used in games, animations, VR/AR.
											Speech Recognition ‚Üí voice assistants, transcription.
											Videoconferencing ‚Üí real-time audio/video encoding/decoding.
											Multimedia Authoring ‚Üí creating and editing video/audio content.
											Voice and Video Annotation of Files ‚Üí adding media notes in documents.
											Simulation Modeling ‚Üí engineering, weather prediction, AI training.




						Designing for Performance ‚Äì Processor Speed & Challenges
1. Increasing Speed of Processors

				Processor speed has historically doubled every 18‚Äì24 months.
				Measured in clock frequency (GHz).
				Higher speed = faster execution, but also more heat and power consumption.

2. Moore‚Äôs Law

				Gordon Moore (Intel co-founder) observed in 1965:
				The number of transistors on a chip doubles roughly every 18‚Äì24 months.

Effect:

				More transistors ‚Üí more circuits in less space.
				Leads to higher performance, lower cost per transistor, and more compact chips.

Example:
				A modern smartphone has billions of transistors, more than early supercomputers.

3. Reducing Distances Between Circuits

				As chips shrink (nanometer technology: 7nm, 5nm, 3nm), the signal travel distance decreases. Shorter distances = faster communication = less delay.

Challenge:

				Very small circuits suffer from heat dissipation and quantum effects (leakage, tunneling).

4. Continuously Feeding Work to Processors

Fast CPUs are useless if they sit idle. The big challenge:How do we keep processors busy at all times?

Techniques:
			Pipelining: Break execution into stages so multiple instructions overlap.
			Superscalar Execution: Multiple instructions issued per clock cycle.
			Out-of-Order Execution: CPU reorders instructions to avoid stalls.
			Branch Prediction: Guess outcomes of branches to avoid waiting.

		Caches: Store recently/frequently used data close to CPU.
				Multithreading / Multicore: Multiple threads or cores run simultaneously.

		Prefetching: Load data before it‚Äôs needed.

what widening a wire actually does

	Lower resistance (R): a thicker/wider conductor has less resistance ‚Üí less voltage drop and less Joule heating for the same current.
	Lower RC delay: resistance √ó capacitance (RC) sets how fast a signal edge rises. Wider traces or on-chip metal layers lower R, so edges are faster ‚Üí lower latency.
	Higher current capacity: wider traces can carry more current without overheating (important for power rails and high-speed parallel drivers).
	But larger area / routing cost: wider wires take space on a die or PCB and use up routing resources (fewer wires fit in a given width).


Make DRAM ‚Äúwider‚Äù instead of ‚Äúdeeper‚Äù

			Deeper memory = more addresses (bigger memory capacity).
			Wider memory = more bits per word (fetching more data per access).

üëâ Example:

		A 1M √ó 8-bit DRAM can store 1 million 8-bit words.											(:=>> Things are same just change the size of the path think ) 
		A 512K √ó 16-bit DRAM has the same total capacity, but each access retrieves 16 bits at once instead of 8.
		So by widening, you don‚Äôt increase the number of words stored, but you increase the width of each access. That means the CPU can get more data per memory cycle.


	Why this helps performance

Modern CPUs can execute billions of instructions per second, but DRAM latency is high (hundreds of cycles).
If each memory access brings more data per cycle, caches and CPU pipelines are less likely to stall.

This improves throughput without necessarily making DRAM itself faster.
 So this is one of the architectural adjustments to handle the mismatch: Increase memory bandwidth by widening DRAM chips and bus data paths, so more bits are delivered to the CPU in each transfer.
Attatched Image shows winder conncetion is availiable in the same path folder view more at Path_Wider.jpg


	One of the architectural tricks used to reduce the mismatch between processor and memory speed.
		Here‚Äôs the idea:
	Problem: The CPU is much faster than DRAM. Every time the processor requests data directly from DRAM, it waits too long ‚Üí bottleneck.
	Solution: Put a small cache or buffer inside the DRAM chip itself. This way, frequently accessed rows/blocks of data can be served faster without going all the way back through the full DRAM access cycle.


	Examples:

		EDO DRAM (Extended Data Out) ‚Üí lets data stay valid longer to speed sequential access.


		SDRAM (Synchronous DRAM) ‚Üí syncs with CPU clock to improve efficiency.
		RDRAM and later DDR SDRAM ‚Üí added internal buffering and pipelining to keep the data bus busy.
		Modern DRAM (e.g., GDDR6, HBM) ‚Üí includes row buffers and prefetch logic (like an internal cache) to transfer wide chunks of data more efficiently.
		
		‚ö° So instead of each request being ‚Äúopen row ‚Üí fetch ‚Üí close row,‚Äù the cache/buffer keeps the row open and serves multiple accesses quickly.

Why reduce memory access?
			
			Accessing DRAM takes hundreds of CPU cycles.
			If the CPU had to go to main memory for every instruction/data fetch, it would spend most of its time waiting.
			So we reduce the number of times CPU must access DRAM ‚Üí use caches.

How caches solve this

			Cache Hierarchy (L1, L2, L3):
			L1 (very small, very fast, per-core).
			L2 (larger, slower, but still faster than RAM).
			L3 (largest, shared across cores).

Each level catches misses from the level above it, reducing how often RAM is touched.

More Efficient Structures:

		Set-associative caches: Smarter placement of data than direct-mapped caches ‚Üí fewer collisions.
		Victim caches: Small extra buffer to store recently evicted lines ‚Üí reduces miss penalties.
		Write-back caches: Update main memory less frequently ‚Üí fewer writes.
		Prefetching: Cache predicts future data and fetches it before CPU requests.
		Inclusive/Exclusive caches: Hierarchy coordination to avoid redundant storage.

Result:
		
		Higher hit ratio ‚Üí CPU finds data in cache instead of RAM.
		Dramatically fewer DRAM accesses ‚Üí faster execution.

Performance Balance

Modern CPUs have become very fast (thanks to Moore‚Äôs Law and high transistor density). But other components‚Äîlike memory and I/O devices‚Äîhaven‚Äôt scaled at the same rate. This creates a mismatch (also called the performance gap).

							CPU speed: nanoseconds (ns)
							Memory (RAM): tens to hundreds of ns
							Disk I/O (HDD/SSD): microseconds (¬µs) to milliseconds (ms)
							Network I/O: even higher latency compared to CPU
I/O Devices in Performance Balance

						CPU vs. I/O ‚Üí CPU executes billions of instructions per second, but I/O devices (disk, network, printer) are much slower.
						Bottleneck ‚Üí If CPU waits idly for I/O, performance collapses.
						Solution ‚Üí Introduce techniques to balance speed differences:
						Buffers & Caches ‚Äì Temporary storage between CPU ‚Üî I/O to smooth data transfer.
						DMA (Direct Memory Access) ‚Äì Lets I/O devices transfer data to RAM directly without CPU intervention.
						Interrupts ‚Äì CPU doesn‚Äôt constantly check the I/O device; instead, the device notifies CPU when ready.
						Parallelism ‚Äì Overlap I/O with CPU work (CPU computes while I/O runs in background).
						Queuing & Scheduling ‚Äì Efficient handling of multiple I/O requests.

Performance Balance & I/O Devices

1. Increasing Demands

As processors get faster, applications become more data-intensive (e.g., video editing, gaming, AI, cloud services).
These require peripherals with high I/O throughput (disks, networks, GPUs, sensors).

2. The Core Problem

Processors can handle massive data rates, but the bottleneck is moving data efficiently between CPU ‚Üî RAM ‚Üî I/O devices.
Mismatch in speed creates latency and throughput issues.

3. Strategies for Balance
	A.	Caching & Buffering
			Temporary storage between processor and device.
					Smooths out bursts in data flow.

	B.	High-Speed Interconnection Buses
				Faster buses (e.g., PCIe, NVMe) reduce transfer delays.
					Bus hierarchies distribute traffic efficiently.

	C.	Bus Structures
				Use of multiple, more elaborate bus systems to handle parallel data movement.
		Example: Separate memory bus and I/O bus.

	D.   Multiprocessor Configurations
					Dedicated processors or controllers handle I/O, reducing CPU load.
			Example: GPU for graphics, NIC offload engines for networking, storage controllers.
