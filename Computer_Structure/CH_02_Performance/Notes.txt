Designing for Performance
1. What is Performance?
    Performance = how fast a computer executes programs.
  Higher performance = less time to complete a task.

Often measured by:
  Response Time (Latency): Time taken to finish one task.

Measuring Performance

Execution Time (T):

`````````````````````ùëá  =(Instruction¬†Count) √ó(CPI)√ó (Clock¬†Cycle¬†Time)
                     T=(Instruction¬†Count)√ó(CPI)√ó(Clock¬†Cycle¬†Time)

            Instruction Count (IC): How many instructions a program needs.
            CPI (Cycles per Instruction): Average cycles per instruction.
            Clock Cycle Time: Inverse of clock rate (1 / frequency).

	‚Äã
Improving Performance

To make a computer faster, you can:
			Increase Clock Rate (GHz): Faster cycles ‚Üí faster CPU.
			‚ö† But leads to heating and power issues.
			Reduce CPI: Optimize hardware (pipelining, parallelism, better instruction design).
			Reduce Instruction Count (IC): Compiler optimizations, efficient ISA.

4. Benchmarks
			Standard programs used to compare systems (e.g., SPEC benchmarks).Real workloads are better indicators than synthetic ones.

5. Amdahl‚Äôs Law (Important)
			The speedup gained from improving part of a system is limited by the portion not improved.

Designing for Performance (Extended Notes)
		Falling Cost, Rising Power
		The cost of computer systems keeps dropping dramatically.Meanwhile, performance and capacity rise equally dramatically.
Example:	
		A laptop today ‚âà the power of an IBM mainframe from 10‚Äì15 years ago.What used to require a multi-million-dollar machine now fits in your backpack.


						Applications Driving Performance Needs
											Modern desktop applications demand the great power of today‚Äôs processors:
											Image Processing ‚Üí e.g., editing high-res photos, filters.
											3D Rendering ‚Üí used in games, animations, VR/AR.
											Speech Recognition ‚Üí voice assistants, transcription.
											Videoconferencing ‚Üí real-time audio/video encoding/decoding.
											Multimedia Authoring ‚Üí creating and editing video/audio content.
											Voice and Video Annotation of Files ‚Üí adding media notes in documents.
											Simulation Modeling ‚Üí engineering, weather prediction, AI training.




						Designing for Performance ‚Äì Processor Speed & Challenges
1. Increasing Speed of Processors

				Processor speed has historically doubled every 18‚Äì24 months.
				Measured in clock frequency (GHz).
				Higher speed = faster execution, but also more heat and power consumption.

2. Moore‚Äôs Law

				Gordon Moore (Intel co-founder) observed in 1965:
				The number of transistors on a chip doubles roughly every 18‚Äì24 months.

Effect:

				More transistors ‚Üí more circuits in less space.
				Leads to higher performance, lower cost per transistor, and more compact chips.

Example:
				A modern smartphone has billions of transistors, more than early supercomputers.

3. Reducing Distances Between Circuits

				As chips shrink (nanometer technology: 7nm, 5nm, 3nm), the signal travel distance decreases. Shorter distances = faster communication = less delay.

Challenge:

				Very small circuits suffer from heat dissipation and quantum effects (leakage, tunneling).

4. Continuously Feeding Work to Processors

Fast CPUs are useless if they sit idle. The big challenge:How do we keep processors busy at all times?

Techniques:
			Pipelining: Break execution into stages so multiple instructions overlap.
			Superscalar Execution: Multiple instructions issued per clock cycle.
			Out-of-Order Execution: CPU reorders instructions to avoid stalls.
			Branch Prediction: Guess outcomes of branches to avoid waiting.

		Caches: Store recently/frequently used data close to CPU.
				Multithreading / Multicore: Multiple threads or cores run simultaneously.

		Prefetching: Load data before it‚Äôs needed.

what widening a wire actually does

	Lower resistance (R): a thicker/wider conductor has less resistance ‚Üí less voltage drop and less Joule heating for the same current.
	Lower RC delay: resistance √ó capacitance (RC) sets how fast a signal edge rises. Wider traces or on-chip metal layers lower R, so edges are faster ‚Üí lower latency.
	Higher current capacity: wider traces can carry more current without overheating (important for power rails and high-speed parallel drivers).
	But larger area / routing cost: wider wires take space on a die or PCB and use up routing resources (fewer wires fit in a given width).


Make DRAM ‚Äúwider‚Äù instead of ‚Äúdeeper‚Äù

			Deeper memory = more addresses (bigger memory capacity).
			Wider memory = more bits per word (fetching more data per access).

üëâ Example:

		A 1M √ó 8-bit DRAM can store 1 million 8-bit words.											(:=>> Things are same just change the size of the path think ) 
		A 512K √ó 16-bit DRAM has the same total capacity, but each access retrieves 16 bits at once instead of 8.
		So by widening, you don‚Äôt increase the number of words stored, but you increase the width of each access. That means the CPU can get more data per memory cycle.


	Why this helps performance

Modern CPUs can execute billions of instructions per second, but DRAM latency is high (hundreds of cycles).
If each memory access brings more data per cycle, caches and CPU pipelines are less likely to stall.

This improves throughput without necessarily making DRAM itself faster.
 So this is one of the architectural adjustments to handle the mismatch: Increase memory bandwidth by widening DRAM chips and bus data paths, so more bits are delivered to the CPU in each transfer.
Attatched Image shows winder conncetion is availiable in the same path folder view more at Path_Wider.jpg


	One of the architectural tricks used to reduce the mismatch between processor and memory speed.
		Here‚Äôs the idea:
	Problem: The CPU is much faster than DRAM. Every time the processor requests data directly from DRAM, it waits too long ‚Üí bottleneck.
	Solution: Put a small cache or buffer inside the DRAM chip itself. This way, frequently accessed rows/blocks of data can be served faster without going all the way back through the full DRAM access cycle.


	Examples:

		EDO DRAM (Extended Data Out) ‚Üí lets data stay valid longer to speed sequential access.


		SDRAM (Synchronous DRAM) ‚Üí syncs with CPU clock to improve efficiency.
		RDRAM and later DDR SDRAM ‚Üí added internal buffering and pipelining to keep the data bus busy.
		Modern DRAM (e.g., GDDR6, HBM) ‚Üí includes row buffers and prefetch logic (like an internal cache) to transfer wide chunks of data more efficiently.
		
		‚ö° So instead of each request being ‚Äúopen row ‚Üí fetch ‚Üí close row,‚Äù the cache/buffer keeps the row open and serves multiple accesses quickly.

Why reduce memory access?
			
			Accessing DRAM takes hundreds of CPU cycles.
			If the CPU had to go to main memory for every instruction/data fetch, it would spend most of its time waiting.
			So we reduce the number of times CPU must access DRAM ‚Üí use caches.
