Designing for Performance
1. What is Performance?
    Performance = how fast a computer executes programs.
  Higher performance = less time to complete a task.

Often measured by:
  Response Time (Latency): Time taken to finish one task.

Measuring Performance

Execution Time (T):

`````````````````````ð‘‡  =(InstructionÂ Count) Ã—(CPI)Ã— (ClockÂ CycleÂ Time)
                     T=(InstructionÂ Count)Ã—(CPI)Ã—(ClockÂ CycleÂ Time)

            Instruction Count (IC): How many instructions a program needs.
            CPI (Cycles per Instruction): Average cycles per instruction.
            Clock Cycle Time: Inverse of clock rate (1 / frequency).

	â€‹
Improving Performance

To make a computer faster, you can:
			Increase Clock Rate (GHz): Faster cycles â†’ faster CPU.
			âš  But leads to heating and power issues.
			Reduce CPI: Optimize hardware (pipelining, parallelism, better instruction design).
			Reduce Instruction Count (IC): Compiler optimizations, efficient ISA.

4. Benchmarks
			Standard programs used to compare systems (e.g., SPEC benchmarks).Real workloads are better indicators than synthetic ones.

5. Amdahlâ€™s Law (Important)
			The speedup gained from improving part of a system is limited by the portion not improved.

Designing for Performance (Extended Notes)
		Falling Cost, Rising Power
		The cost of computer systems keeps dropping dramatically.Meanwhile, performance and capacity rise equally dramatically.
Example:	
		A laptop today â‰ˆ the power of an IBM mainframe from 10â€“15 years ago.What used to require a multi-million-dollar machine now fits in your backpack.


						Applications Driving Performance Needs
											Modern desktop applications demand the great power of todayâ€™s processors:
											Image Processing â†’ e.g., editing high-res photos, filters.
											3D Rendering â†’ used in games, animations, VR/AR.
											Speech Recognition â†’ voice assistants, transcription.
											Videoconferencing â†’ real-time audio/video encoding/decoding.
											Multimedia Authoring â†’ creating and editing video/audio content.
											Voice and Video Annotation of Files â†’ adding media notes in documents.
											Simulation Modeling â†’ engineering, weather prediction, AI training.




						Designing for Performance â€“ Processor Speed & Challenges
1. Increasing Speed of Processors

				Processor speed has historically doubled every 18â€“24 months.
				Measured in clock frequency (GHz).
				Higher speed = faster execution, but also more heat and power consumption.

2. Mooreâ€™s Law

				Gordon Moore (Intel co-founder) observed in 1965:
				The number of transistors on a chip doubles roughly every 18â€“24 months.

Effect:

				More transistors â†’ more circuits in less space.
				Leads to higher performance, lower cost per transistor, and more compact chips.

Example:
				A modern smartphone has billions of transistors, more than early supercomputers.

3. Reducing Distances Between Circuits

				As chips shrink (nanometer technology: 7nm, 5nm, 3nm), the signal travel distance decreases. Shorter distances = faster communication = less delay.

Challenge:

				Very small circuits suffer from heat dissipation and quantum effects (leakage, tunneling).

4. Continuously Feeding Work to Processors

Fast CPUs are useless if they sit idle. The big challenge:How do we keep processors busy at all times?

Techniques:
			Pipelining: Break execution into stages so multiple instructions overlap.
			Superscalar Execution: Multiple instructions issued per clock cycle.
			Out-of-Order Execution: CPU reorders instructions to avoid stalls.
			Branch Prediction: Guess outcomes of branches to avoid waiting.

		Caches: Store recently/frequently used data close to CPU.
				Multithreading / Multicore: Multiple threads or cores run simultaneously.

		Prefetching: Load data before itâ€™s needed.

what widening a wire actually does

	Lower resistance (R): a thicker/wider conductor has less resistance â†’ less voltage drop and less Joule heating for the same current.
	Lower RC delay: resistance Ã— capacitance (RC) sets how fast a signal edge rises. Wider traces or on-chip metal layers lower R, so edges are faster â†’ lower latency.
	Higher current capacity: wider traces can carry more current without overheating (important for power rails and high-speed parallel drivers).
	But larger area / routing cost: wider wires take space on a die or PCB and use up routing resources (fewer wires fit in a given width).


Make DRAM â€œwiderâ€ instead of â€œdeeperâ€

			Deeper memory = more addresses (bigger memory capacity).
			Wider memory = more bits per word (fetching more data per access).

ðŸ‘‰ Example:

		A 1M Ã— 8-bit DRAM can store 1 million 8-bit words.											(:=>> Things are same just change the size of the path think ) 
		A 512K Ã— 16-bit DRAM has the same total capacity, but each access retrieves 16 bits at once instead of 8.
		So by widening, you donâ€™t increase the number of words stored, but you increase the width of each access. That means the CPU can get more data per memory cycle.


	Why this helps performance

Modern CPUs can execute billions of instructions per second, but DRAM latency is high (hundreds of cycles).
If each memory access brings more data per cycle, caches and CPU pipelines are less likely to stall.

This improves throughput without necessarily making DRAM itself faster.
 So this is one of the architectural adjustments to handle the mismatch: Increase memory bandwidth by widening DRAM chips and bus data paths, so more bits are delivered to the CPU in each transfer.
Attatched Image shows winder conncetion is availiable in the same path folder view more at Path_Wider.jpg


	One of the architectural tricks used to reduce the mismatch between processor and memory speed.
		Hereâ€™s the idea:
	Problem: The CPU is much faster than DRAM. Every time the processor requests data directly from DRAM, it waits too long â†’ bottleneck.
	Solution: Put a small cache or buffer inside the DRAM chip itself. This way, frequently accessed rows/blocks of data can be served faster without going all the way back through the full DRAM access cycle.


	Examples:

		EDO DRAM (Extended Data Out) â†’ lets data stay valid longer to speed sequential access.


		SDRAM (Synchronous DRAM) â†’ syncs with CPU clock to improve efficiency.
		RDRAM and later DDR SDRAM â†’ added internal buffering and pipelining to keep the data bus busy.
		Modern DRAM (e.g., GDDR6, HBM) â†’ includes row buffers and prefetch logic (like an internal cache) to transfer wide chunks of data more efficiently.
		
		âš¡ So instead of each request being â€œopen row â†’ fetch â†’ close row,â€ the cache/buffer keeps the row open and serves multiple accesses quickly.

Why reduce memory access?
			
			Accessing DRAM takes hundreds of CPU cycles.
			If the CPU had to go to main memory for every instruction/data fetch, it would spend most of its time waiting.
			So we reduce the number of times CPU must access DRAM â†’ use caches.

How caches solve this

			Cache Hierarchy (L1, L2, L3):
			L1 (very small, very fast, per-core).
			L2 (larger, slower, but still faster than RAM).
			L3 (largest, shared across cores).

Each level catches misses from the level above it, reducing how often RAM is touched.

More Efficient Structures:

		Set-associative caches: Smarter placement of data than direct-mapped caches â†’ fewer collisions.
		Victim caches: Small extra buffer to store recently evicted lines â†’ reduces miss penalties.
		Write-back caches: Update main memory less frequently â†’ fewer writes.
		Prefetching: Cache predicts future data and fetches it before CPU requests.
		Inclusive/Exclusive caches: Hierarchy coordination to avoid redundant storage.

Result:
		
		Higher hit ratio â†’ CPU finds data in cache instead of RAM.
		Dramatically fewer DRAM accesses â†’ faster execution.

Performance Balance

Modern CPUs have become very fast (thanks to Mooreâ€™s Law and high transistor density). But other componentsâ€”like memory and I/O devicesâ€”havenâ€™t scaled at the same rate. This creates a mismatch (also called the performance gap).

							CPU speed: nanoseconds (ns)
							Memory (RAM): tens to hundreds of ns
							Disk I/O (HDD/SSD): microseconds (Âµs) to milliseconds (ms)
							Network I/O: even higher latency compared to CPU
I/O Devices in Performance Balance

						CPU vs. I/O â†’ CPU executes billions of instructions per second, but I/O devices (disk, network, printer) are much slower.
						Bottleneck â†’ If CPU waits idly for I/O, performance collapses.
						Solution â†’ Introduce techniques to balance speed differences:
						Buffers & Caches â€“ Temporary storage between CPU â†” I/O to smooth data transfer.
						DMA (Direct Memory Access) â€“ Lets I/O devices transfer data to RAM directly without CPU intervention.
						Interrupts â€“ CPU doesnâ€™t constantly check the I/O device; instead, the device notifies CPU when ready.
						Parallelism â€“ Overlap I/O with CPU work (CPU computes while I/O runs in background).
						Queuing & Scheduling â€“ Efficient handling of multiple I/O requests.

Performance Balance & I/O Devices

1. Increasing Demands

As processors get faster, applications become more data-intensive (e.g., video editing, gaming, AI, cloud services).
These require peripherals with high I/O throughput (disks, networks, GPUs, sensors).

2. The Core Problem

Processors can handle massive data rates, but the bottleneck is moving data efficiently between CPU â†” RAM â†” I/O devices.
Mismatch in speed creates latency and throughput issues.

3. Strategies for Balance
	A.	Caching & Buffering
			Temporary storage between processor and device.
					Smooths out bursts in data flow.

	B.	High-Speed Interconnection Buses
				Faster buses (e.g., PCIe, NVMe) reduce transfer delays.
					Bus hierarchies distribute traffic efficiently.

	C.	Bus Structures
				Use of multiple, more elaborate bus systems to handle parallel data movement.
		Example: Separate memory bus and I/O bus.

	D.   Multiprocessor Configurations
					Dedicated processors or controllers handle I/O, reducing CPU load.
			Example: GPU for graphics, NIC offload engines for networking, storage controllers.



														Improvements in Chip Organization & Architecture

As processor speeds have increased rapidly, chip designers had to reorganize and innovate architectures to keep performance balanced across the system.

1. Increasing Speed of Processors
			Mooreâ€™s Law â†’ more transistors in less space â†’ faster and more capable processors.
			Example: From millions to billions of transistors on modern CPUs.

2. Reducing Distances Between Circuits
			Smaller semiconductor technology (nanometer scale).
			Shorter paths â†’ reduced signal travel time â†’ higher clock rates.
			Example: 7nm â†’ 5nm â†’ 3nm chips.

3. Cache Memory Hierarchy
			Multi-level caches (L1, L2, L3) added between CPU and RAM.
			Reduces memory access time by keeping frequently used data closer to CPU.
			Hit/Miss optimization ensures CPU doesnâ€™t stall waiting for slow RAM.

4. Pipelining & Instruction-Level Parallelism
			Divide execution into stages (fetch, decode, execute, etc.).
			Multiple instructions processed at once (assembly line model).
			Superscalar architectures â†’ multiple instructions per cycle.

5. Multicore Processors
			Instead of just making one core faster, chips now integrate multiple cores.
			Parallelism improves throughput for multitasking and I/O-heavy workloads.
			Example: Quad-core, Octa-core CPUs.

6. Specialized Processing Units
			Integration of GPUs, AI accelerators, and network controllers on-chip.
			System on Chip (SoC) designs combine CPU, memory controllers, I/O interfaces in one package.

7. High-Speed Interconnections
			Faster buses (PCIe, NVLink, Infinity Fabric).
			On-chip interconnect networks (NoCs â€“ Networks on Chip).
			Reduce bottlenecks between CPU, cache, RAM, and I/O devices.

8. Power & Thermal Management

			Chips optimized for performance per watt.
			Techniques: Dynamic Voltage & Frequency Scaling (DVFS), low-power cores (big.LITTLE).




													Problems with Clock Speed and Logic Density
1. Clock Speed Problems
		A. Heat Dissipation
				Higher clock speeds = more power consumption.
				More power â†’ more heat â†’ harder to cool.
				Example: CPUs above ~4â€“5 GHz face thermal limits.

		B.  Power Consumption
				Power âˆ VoltageÂ² Ã— Frequency.
				Increasing clock frequency dramatically raises power needs.

		C. Signal Delay (Propagation Delay)
				At very high frequencies, even small wire distances inside chips cause timing errors.
		D. Diminishing Returns
				Doubling clock speed doesnâ€™t double performance (bottlenecks in memory, I/O).
Summary of clock time limitations:=>> 
			Clock Speed Limits â†’ heat, power, timing problems.
			Logic Density Limits â†’ leakage, fabrication, interconnect bottlenecks.
			Together, they forced a shift to new architectures instead of just scaling speed and densit
