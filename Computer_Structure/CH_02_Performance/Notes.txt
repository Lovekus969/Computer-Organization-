Designing for Performance
1. What is Performance?
    Performance = how fast a computer executes programs.
  Higher performance = less time to complete a task.

Often measured by:
  Response Time (Latency): Time taken to finish one task.

Measuring Performance

Execution Time (T):

`````````````````````ð‘‡  =(InstructionÂ Count) Ã—(CPI)Ã— (ClockÂ CycleÂ Time)
                     T=(InstructionÂ Count)Ã—(CPI)Ã—(ClockÂ CycleÂ Time)

            Instruction Count (IC): How many instructions a program needs.
            CPI (Cycles per Instruction): Average cycles per instruction.
            Clock Cycle Time: Inverse of clock rate (1 / frequency).

	â€‹
Improving Performance

To make a computer faster, you can:
			Increase Clock Rate (GHz): Faster cycles â†’ faster CPU.
			âš  But leads to heating and power issues.
			Reduce CPI: Optimize hardware (pipelining, parallelism, better instruction design).
			Reduce Instruction Count (IC): Compiler optimizations, efficient ISA.

4. Benchmarks
			Standard programs used to compare systems (e.g., SPEC benchmarks).Real workloads are better indicators than synthetic ones.

5. Amdahlâ€™s Law (Important)
			The speedup gained from improving part of a system is limited by the portion not improved.

Designing for Performance (Extended Notes)
		Falling Cost, Rising Power
		The cost of computer systems keeps dropping dramatically.Meanwhile, performance and capacity rise equally dramatically.
Example:	
		A laptop today â‰ˆ the power of an IBM mainframe from 10â€“15 years ago.What used to require a multi-million-dollar machine now fits in your backpack.


						Applications Driving Performance Needs
											Modern desktop applications demand the great power of todayâ€™s processors:
											Image Processing â†’ e.g., editing high-res photos, filters.
											3D Rendering â†’ used in games, animations, VR/AR.
											Speech Recognition â†’ voice assistants, transcription.
											Videoconferencing â†’ real-time audio/video encoding/decoding.
											Multimedia Authoring â†’ creating and editing video/audio content.
											Voice and Video Annotation of Files â†’ adding media notes in documents.
											Simulation Modeling â†’ engineering, weather prediction, AI training.




						Designing for Performance â€“ Processor Speed & Challenges
1. Increasing Speed of Processors

				Processor speed has historically doubled every 18â€“24 months.
				Measured in clock frequency (GHz).
				Higher speed = faster execution, but also more heat and power consumption.

2. Mooreâ€™s Law

				Gordon Moore (Intel co-founder) observed in 1965:
				The number of transistors on a chip doubles roughly every 18â€“24 months.

Effect:

				More transistors â†’ more circuits in less space.
				Leads to higher performance, lower cost per transistor, and more compact chips.

Example:
				A modern smartphone has billions of transistors, more than early supercomputers.

3. Reducing Distances Between Circuits

				As chips shrink (nanometer technology: 7nm, 5nm, 3nm), the signal travel distance decreases. Shorter distances = faster communication = less delay.

Challenge:

				Very small circuits suffer from heat dissipation and quantum effects (leakage, tunneling).

4. Continuously Feeding Work to Processors

Fast CPUs are useless if they sit idle. The big challenge:How do we keep processors busy at all times?

Techniques:
			Pipelining: Break execution into stages so multiple instructions overlap.
			Superscalar Execution: Multiple instructions issued per clock cycle.
			Out-of-Order Execution: CPU reorders instructions to avoid stalls.
			Branch Prediction: Guess outcomes of branches to avoid waiting.

		Caches: Store recently/frequently used data close to CPU.
				Multithreading / Multicore: Multiple threads or cores run simultaneously.

		Prefetching: Load data before itâ€™s needed.

what widening a wire actually does

	Lower resistance (R): a thicker/wider conductor has less resistance â†’ less voltage drop and less Joule heating for the same current.
	Lower RC delay: resistance Ã— capacitance (RC) sets how fast a signal edge rises. Wider traces or on-chip metal layers lower R, so edges are faster â†’ lower latency.
	Higher current capacity: wider traces can carry more current without overheating (important for power rails and high-speed parallel drivers).
	But larger area / routing cost: wider wires take space on a die or PCB and use up routing resources (fewer wires fit in a given width).


Make DRAM â€œwiderâ€ instead of â€œdeeperâ€

			Deeper memory = more addresses (bigger memory capacity).
			Wider memory = more bits per word (fetching more data per access).

ðŸ‘‰ Example:

		A 1M Ã— 8-bit DRAM can store 1 million 8-bit words.											(:=>> Things are same just change the size of the path think ) 
		A 512K Ã— 16-bit DRAM has the same total capacity, but each access retrieves 16 bits at once instead of 8.
		So by widening, you donâ€™t increase the number of words stored, but you increase the width of each access. That means the CPU can get more data per memory cycle.


	Why this helps performance

Modern CPUs can execute billions of instructions per second, but DRAM latency is high (hundreds of cycles).
If each memory access brings more data per cycle, caches and CPU pipelines are less likely to stall.

This improves throughput without necessarily making DRAM itself faster.
 So this is one of the architectural adjustments to handle the mismatch: Increase memory bandwidth by widening DRAM chips and bus data paths, so more bits are delivered to the CPU in each transfer.
Attatched Image shows winder conncetion is availiable in the same path folder view more at Path_Wider.jpg
